{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np\n",
    "\n",
    "# Functions to read in the corpus\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "  with open(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "      tag, words = line.lower().strip().split(\" ||| \")\n",
    "      yield ([w2i[x] for x in words.split(\" \")], t2i[tag])\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"data/classes/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"data/classes/test.txt\"))\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)\n",
    "\n",
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "HID_LAY = 2\n",
    "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE)) # Word embeddings\n",
    "W_h = [model.add_parameters((HID_SIZE, EMB_SIZE if lay == 0 else HID_SIZE)) for lay in range(HID_LAY)]\n",
    "b_h = [model.add_parameters((HID_SIZE)) for lay in range(HID_LAY)]\n",
    "W_sm = model.add_parameters((ntags, HID_SIZE))          # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))                    # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to calculate scores for one value\n",
    "def calc_scores(words):\n",
    "  dy.renew_cg()\n",
    "  h = dy.esum([dy.lookup(W_emb, x) for x in words])\n",
    "  for W_h_i, b_h_i in zip(W_h, b_h):\n",
    "    h = dy.tanh( dy.parameter(W_h_i) * h + dy.parameter(b_h_i) )\n",
    "  return dy.parameter(W_sm) * h + dy.parameter(b_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=1.5533, time=0.95s\n",
      "iter 0: test acc=0.3801\n",
      "iter 1: train loss/sent=1.2708, time=0.95s\n",
      "iter 1: test acc=0.4158\n",
      "iter 2: train loss/sent=1.0447, time=0.97s\n",
      "iter 2: test acc=0.3919\n",
      "iter 3: train loss/sent=0.8109, time=0.95s\n",
      "iter 3: test acc=0.4023\n",
      "iter 4: train loss/sent=0.6108, time=0.97s\n",
      "iter 4: test acc=0.3860\n",
      "iter 5: train loss/sent=0.4319, time=0.98s\n",
      "iter 5: test acc=0.3923\n",
      "iter 6: train loss/sent=0.3139, time=0.94s\n",
      "iter 6: test acc=0.4005\n",
      "iter 7: train loss/sent=0.2178, time=0.99s\n",
      "iter 7: test acc=0.3869\n",
      "iter 8: train loss/sent=0.1589, time=0.97s\n",
      "iter 8: test acc=0.3905\n",
      "iter 9: train loss/sent=0.1129, time=0.98s\n",
      "iter 9: test acc=0.3842\n",
      "iter 10: train loss/sent=0.0868, time=0.99s\n",
      "iter 10: test acc=0.3819\n",
      "iter 11: train loss/sent=0.0684, time=0.96s\n",
      "iter 11: test acc=0.3910\n",
      "iter 12: train loss/sent=0.0536, time=0.97s\n",
      "iter 12: test acc=0.3824\n",
      "iter 13: train loss/sent=0.0442, time=0.98s\n",
      "iter 13: test acc=0.3910\n",
      "iter 14: train loss/sent=0.0364, time=0.96s\n",
      "iter 14: test acc=0.3824\n",
      "iter 15: train loss/sent=0.0365, time=0.97s\n",
      "iter 15: test acc=0.3833\n",
      "iter 16: train loss/sent=0.0302, time=0.98s\n",
      "iter 16: test acc=0.3760\n",
      "iter 17: train loss/sent=0.0270, time=1.00s\n",
      "iter 17: test acc=0.3769\n",
      "iter 18: train loss/sent=0.0314, time=1.00s\n",
      "iter 18: test acc=0.3937\n",
      "iter 19: train loss/sent=0.0216, time=0.99s\n",
      "iter 19: test acc=0.3729\n",
      "iter 20: train loss/sent=0.0307, time=0.98s\n",
      "iter 20: test acc=0.3959\n",
      "iter 21: train loss/sent=0.0282, time=1.12s\n",
      "iter 21: test acc=0.3860\n",
      "iter 22: train loss/sent=0.0271, time=1.13s\n",
      "iter 22: test acc=0.3869\n",
      "iter 23: train loss/sent=0.0299, time=1.21s\n",
      "iter 23: test acc=0.3855\n",
      "iter 24: train loss/sent=0.0263, time=1.11s\n",
      "iter 24: test acc=0.3747\n",
      "iter 25: train loss/sent=0.0215, time=1.29s\n",
      "iter 25: test acc=0.3878\n",
      "iter 26: train loss/sent=0.0250, time=1.32s\n",
      "iter 26: test acc=0.3751\n",
      "iter 27: train loss/sent=0.0350, time=1.33s\n",
      "iter 27: test acc=0.3787\n",
      "iter 28: train loss/sent=0.0271, time=1.22s\n",
      "iter 28: test acc=0.3706\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-855f5b795609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickneglogsoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d9329d81132e>\u001b[0m in \u001b[0;36mcalc_scores\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenew_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mesum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mW_h_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_h_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_h_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_h_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ITER in range(100):\n",
    "  # Perform training\n",
    "  random.shuffle(train)\n",
    "  train_loss = 0.0\n",
    "  start = time.time()\n",
    "  for words, tag in train:\n",
    "    my_loss = dy.pickneglogsoftmax(calc_scores(words), tag)\n",
    "    train_loss += my_loss.value()\n",
    "    my_loss.backward()\n",
    "    trainer.update()\n",
    "  print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss/len(train), time.time()-start))\n",
    "  # Perform testing\n",
    "  test_correct = 0.0\n",
    "  for words, tag in dev:\n",
    "    scores = calc_scores(words).npvalue()\n",
    "    predict = np.argmax(scores)\n",
    "    if predict == tag:\n",
    "      test_correct += 1\n",
    "  print(\"iter %r: test acc=%.4f\" % (ITER, test_correct/len(dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
